{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple backpropagation in Devito"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we'll implement a simple convolutional neural network (CNN) in Devito, run a forward pass through it and then use backpropagation to obtain gradients necessary for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CNN will have the following structure:\n",
    "1. Max pool layer: input size 1x2x4x4, kernel size 2x2, stride 1x1\n",
    "2. Convolutional layer: input size 1x2x3x3, kernel size 2x2x2, stride 1x1, activation ReLU\n",
    "3. Flattening layer\n",
    "4. Fully connected layer: input size 8x1, kernel size 3x8, activation softmax\n",
    "\n",
    "*Size glossary: batch size x channels x height x width **or** output channels x height x width **or** height x width. Height and width are equivalent to rows and columns.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All parameters for the forward pass will be (pseudo)random numbers generated by `np.random.rand`. Therefore, different results will be obtained each time the notebook is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joey\n",
    "import numpy as np\n",
    "from sympy.functions import sign\n",
    "from devito import Operator, Eq, Inc\n",
    "from joey.activation import ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(conv_kernel, conv_bias, fc_weights, fc_bias, input_data, expected_results):\n",
    "    layer1 = joey.MaxPooling(kernel_size=(2, 2),\n",
    "                             input_size=(1, 2, 4, 4),\n",
    "                             generate_code=False)\n",
    "    layer2 = joey.Conv(kernel_size=(2, 2, 2),\n",
    "                       input_size=(1, 2, 3, 3),\n",
    "                       activation=ReLU(),\n",
    "                       generate_code=False)\n",
    "    layer_flat = joey.Flat(input_size=(1, 2, 2, 2),\n",
    "                           generate_code=False)\n",
    "    layer3 = joey.FullyConnectedSoftmax(weight_size=(3, 8),\n",
    "                                        input_size=(8, 1),\n",
    "                                        generate_code=False)\n",
    "    \n",
    "    eqs = layer1.equations() + layer2.equations(layer1.result) + \\\n",
    "            layer_flat.equations(layer2.result) + layer3.equations(layer_flat.result)\n",
    "    \n",
    "    op = Operator(eqs)\n",
    "    \n",
    "    layer2.kernel.data[:] = conv_kernel\n",
    "    layer2.bias.data[:] = conv_bias\n",
    "    \n",
    "    layer3.kernel.data[:] = fc_weights\n",
    "    layer3.bias.data[:] = fc_bias\n",
    "    \n",
    "    layer1.input.data[:] = input_data\n",
    "    \n",
    "    op.apply()\n",
    "    \n",
    "    gradients = []\n",
    "    \n",
    "    for i in range(3):\n",
    "        result = layer3.result.data[i]\n",
    "        if i == expected_results[0]:\n",
    "            result -= 1\n",
    "        gradients.append(result)\n",
    "    \n",
    "    layer3.result_gradients.data[:] = gradients\n",
    "    \n",
    "    dims = [layer3.bias_gradients.dimensions[0],\n",
    "            layer3.kernel_gradients.dimensions[0],\n",
    "            layer3.kernel_gradients.dimensions[1],\n",
    "            layer_flat.result_gradients.dimensions[0],\n",
    "            layer3.kernel.dimensions[1],\n",
    "            layer3.result_gradients.dimensions[0],\n",
    "            layer2.result_gradients.dimensions[0],\n",
    "            layer2.result_gradients.dimensions[1],\n",
    "            layer2.result_gradients.dimensions[2],\n",
    "            layer2.kernel_gradients.dimensions[0],\n",
    "            layer2.kernel_gradients.dimensions[1],\n",
    "            layer2.kernel_gradients.dimensions[2],\n",
    "            layer2.kernel_gradients.dimensions[3],\n",
    "            layer2.bias_gradients.dimensions[0]]\n",
    "    \n",
    "    _, _, layer2_height, layer2_width = layer2.kernel.shape\n",
    "    \n",
    "    backprop_eqs = [\n",
    "        Eq(layer3.bias_gradients[dims[0]], layer3.result_gradients[dims[0]]),\n",
    "        Eq(layer3.kernel_gradients[dims[1], dims[2]],\n",
    "           layer_flat.result[dims[2], 0] * layer3.result_gradients[dims[1]]),\n",
    "        Inc(layer_flat.result_gradients[dims[4]],\n",
    "            layer3.kernel[dims[5], dims[4]] * layer3.result_gradients[dims[5]]),\n",
    "        Eq(layer_flat.result_gradients[dims[3]],\n",
    "           layer_flat.result_gradients[dims[3]] * sign(layer_flat.result[dims[3], 0])),\n",
    "        Eq(layer2.result_gradients[dims[6], dims[7], dims[8]],\n",
    "           layer_flat.result_gradients[dims[6] * layer2_height * layer2_width + dims[7] * layer2_height + dims[8]]),\n",
    "        Inc(layer2.bias_gradients[dims[13]], layer2.result_gradients[dims[13], dims[7], dims[8]]),\n",
    "        Eq(layer2.kernel_gradients[dims[9], dims[10], dims[11], dims[12]],\n",
    "            sum([layer2.result_gradients[dims[9], x, y] * layer1.result[0, dims[10], dims[11] + x, dims[12] + y]\n",
    "                 for x in range(layer2.result_gradients.shape[1])\n",
    "                 for y in range(layer2.result_gradients.shape[2])]))\n",
    "    ]\n",
    "    \n",
    "    backprop_op = Operator(backprop_eqs)\n",
    "    backprop_op.apply()\n",
    "    \n",
    "    return (layer2.kernel_gradients.data, layer2.bias_gradients.data,\n",
    "            layer3.kernel_gradients.data, layer3.bias_gradients.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_kernel = np.random.rand(2, 2, 2)\n",
    "conv_bias = np.random.rand(2)\n",
    "\n",
    "fc_weights = np.random.rand(3, 8)\n",
    "fc_bias = np.random.rand(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = np.array([[[[1, 2, 3, 4],\n",
    "                         [5, 6, 7, 8],\n",
    "                         [9, 10, 11, 12],\n",
    "                         [13, 14, 15, 16]],\n",
    "                        [[-1, -2, 0, 1],\n",
    "                         [-2, -3, 1, 2],\n",
    "                         [3, 4, 2, -1],\n",
    "                         [-2, -3, -4, 9]]]],\n",
    "                     dtype=np.float64)\n",
    "expected = np.array([2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maksymilian/Desktop/UROP/devito/devito/types/grid.py:206: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  spacing = (np.array(self.extent) / (np.array(self.shape) - 1)).astype(self.dtype)\n",
      "Operator `Kernel` run in 0.01 s\n",
      "Operator `Kernel` run in 0.01 s\n"
     ]
    }
   ],
   "source": [
    "conv_kernel_grad, conv_bias_grad, fc_kernel_grad, fc_bias_grad = backward_pass(conv_kernel, conv_bias, fc_weights, fc_bias, input_data, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are kernel gradients of the convolutional layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 1.77081810e-10  2.19350411e-10]\n",
      "   [ 3.46156214e-10  3.88424815e-10]]\n",
      "\n",
      "  [[-1.00338299e-10  1.88046451e-11]\n",
      "   [ 1.69074404e-10  5.35400815e-11]]]\n",
      "\n",
      "\n",
      " [[[-6.81563531e-10 -7.66421514e-10]\n",
      "   [-1.02099546e-09 -1.10585345e-09]]\n",
      "\n",
      "  [[-1.29241071e-10 -1.51084775e-10]\n",
      "   [-3.39431933e-10 -3.87942350e-10]]]]\n"
     ]
    }
   ],
   "source": [
    "print(conv_kernel_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are bias gradients of the convolutional layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.22686009e-11 -8.48579832e-11]\n"
     ]
    }
   ],
   "source": [
    "print(conv_bias_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are kernel gradients of the fully connected layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.36506389e-14  2.71186908e-14  3.67867182e-14  3.78706765e-14\n",
      "   2.36562622e-14  2.71243141e-14  3.67923415e-14  3.78762998e-14]\n",
      " [ 8.17835348e-10  9.37760035e-10  1.27207889e-09  1.30956200e-09\n",
      "   8.18029802e-10  9.37954489e-10  1.27227334e-09  1.30975645e-09]\n",
      " [-8.17857696e-10 -9.37785659e-10 -1.27211365e-09 -1.30959778e-09\n",
      "  -8.18052155e-10 -9.37980118e-10 -1.27230811e-09 -1.30979224e-09]]\n"
     ]
    }
   ],
   "source": [
    "print(fc_kernel_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are bias gradients of the fully connected layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.45661929e-15  5.03696643e-11 -5.03710407e-11]\n"
     ]
    }
   ],
   "source": [
    "print(fc_bias_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with PyTorch\n",
    "To check correctness, we will carry out the same activities using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv = nn.Conv2d(2, 2, 2)\n",
    "        self.fc = nn.Linear(8, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(x, 2, stride=(1, 1))\n",
    "        x = F.relu(self.conv(x))\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net.double()\n",
    "\n",
    "with torch.no_grad():\n",
    "    net.conv.weight[:] = torch.from_numpy(conv_kernel)\n",
    "    net.conv.bias[:] = torch.from_numpy(conv_bias)\n",
    "    \n",
    "    net.fc.weight[:] = torch.from_numpy(fc_weights)\n",
    "    net.fc.bias[:] = torch.from_numpy(fc_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "input_data_tensor = torch.from_numpy(input_data)\n",
    "outputs = net(input_data_tensor)\n",
    "net.zero_grad()\n",
    "loss = criterion(outputs, torch.from_numpy(expected))\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the relative convolutional layer kernel error along with the maximum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[6.62723345e-14 6.04545902e-14]\n",
      "   [5.15261066e-14 4.97788661e-14]]\n",
      "\n",
      "  [[1.72607014e-14 1.41930354e-13]\n",
      "   [3.59286059e-14 1.23115156e-13]]]\n",
      "\n",
      "\n",
      " [[[9.40579925e-15 9.30875850e-15]\n",
      "   [9.51950576e-15 9.53702576e-15]]\n",
      "\n",
      "  [[8.00036521e-15 1.04366111e-14]\n",
      "   [9.59551341e-15 1.01280716e-14]]]]\n",
      "1.419303541865712e-13\n"
     ]
    }
   ],
   "source": [
    "pytorch_conv_kernel_grad = net.conv.weight.grad.numpy()\n",
    "conv_kernel_error = abs(conv_kernel_grad - pytorch_conv_kernel_grad) / abs(pytorch_conv_kernel_grad)\n",
    "print(conv_kernel_error)\n",
    "print(np.amax(conv_kernel_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the relative convolutional layer bias error along with the maximum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.59286059e-14 9.44320367e-15]\n",
      "3.592860592048525e-14\n"
     ]
    }
   ],
   "source": [
    "pytorch_conv_bias_grad = net.conv.bias.grad.numpy()\n",
    "conv_bias_error = abs(conv_bias_grad - pytorch_conv_bias_grad) / abs(pytorch_conv_bias_grad)\n",
    "print(conv_bias_error)\n",
    "print(np.amax(conv_bias_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the relative fully connected layer kernel error along with the maximum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.36087338e-14 1.34973868e-14 1.38958269e-14 1.36647349e-14\n",
      "  1.37388861e-14 1.34945886e-14 1.37221758e-14 1.36627061e-14]\n",
      " [1.37806905e-14 1.34517402e-14 1.38180015e-14 1.37383174e-14\n",
      "  1.37774147e-14 1.36694260e-14 1.39784294e-14 1.37362777e-14]\n",
      " [0.00000000e+00 2.20514305e-16 1.62560281e-16 0.00000000e+00\n",
      "  0.00000000e+00 2.20468589e-16 1.62535436e-16 0.00000000e+00]]\n",
      "1.3978429432582165e-14\n"
     ]
    }
   ],
   "source": [
    "pytorch_fc_kernel_grad = net.fc.weight.grad.numpy()\n",
    "fc_kernel_error = abs(fc_kernel_grad - pytorch_fc_kernel_grad) / abs(pytorch_fc_kernel_grad)\n",
    "print(fc_kernel_error)\n",
    "print(np.amax(fc_kernel_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the relative fully connected layer bias error along with the maximum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.36746355e-14 1.37279313e-14 0.00000000e+00]\n",
      "1.3727931341179665e-14\n"
     ]
    }
   ],
   "source": [
    "pytorch_fc_bias_grad = net.fc.bias.grad.numpy()\n",
    "fc_bias_error = abs(fc_bias_grad - pytorch_fc_bias_grad) / abs(pytorch_fc_bias_grad)\n",
    "print(fc_bias_error)\n",
    "print(np.amax(fc_bias_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the maximum overall error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.419303541865712e-13\n"
     ]
    }
   ],
   "source": [
    "print(max(np.amax(conv_kernel_error), np.amax(conv_bias_error), np.amax(fc_kernel_error), np.amax(fc_bias_error)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
